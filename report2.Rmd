---
title: "Analysis of Online Shopping Behavior"
author: "Despoina Iapona & Gerard Palomo"
output:
  pdf_document:
    toc: true
    toc_depth: 3
date: "`r Sys.Date()`"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, out.height = '70%', out.width = '70%', fig.align = "center", comment = "#>")
```


```{r}
library(ggplot2)
library(dplyr)
library(cluster)
library(reshape2)
library(Rcpp)
library(gridExtra)
library(clustMixType)
```


```{r, include=FALSE}
# Import data
data <- readRDS('data/R_datasets/processed_dataset.rds')

# Data wrangling
data <- data %>% 
  mutate(
    Weekend = as.numeric(as.character(Weekend)),
    Revenue = as.numeric(as.character(Revenue)),
    SpecialDay = as.numeric(SpecialDay),
    VisitorType = as.numeric(VisitorType),
    Month = as.numeric(Month),
    OperatingSystems = as.numeric(as.character(OperatingSystems)),
    Browser = as.numeric(as.character(Browser)),
    Region = as.numeric(as.character(Region)),
    TrafficType = as.numeric(as.character(TrafficType))
  ) %>%
  .[, c(
    "Administrative", "AdministrativeDuration", "Informational", "InformationalDuration", 
    "ProductRelated", "ProductRelatedDuration", "BounceRates", "ExitRates", "PageValues", 
    "Weekend", "Revenue", "SpecialDay", "Month", "OperatingSystems", "Browser", "Region", 
    "TrafficType", "VisitorType"
  )] %>%
  sample_n(1000)  # Randomly sample 1000 observations

# Data without revenue variable
data_no_revenue <- data %>% select(-Revenue)

str(data)
```

```{r}
continuous_data = data[, c(
  "Administrative", "AdministrativeDuration", "Informational", "InformationalDuration", 
  "ProductRelated", "ProductRelatedDuration", "BounceRates", "ExitRates", "PageValues"
)]
binary_data = data[, c("Weekend", "Revenue")]
categorical_data = data[, c("SpecialDay", "Month", "OperatingSystems", "Browser", "Region", "TrafficType", "VisitorType")]
```

# Multidimenional Scaling

Multidimensional Scaling (MDS) is a powerful technique used to visualize complex data by arranging points in a low-dimensional Euclidean space. Unlike Principal Component Analysis (PCA), which works on raw data, MDS operates on a **distance matrix**, making it suitable for various types of data, including binary, categorical, and quantitative.

The goal of MDS is to find a configuration of points in a lower-dimensional space that best preserves the pairwise distances from the original distance matrix. This allows us to visually explore the relationships between observations in a way that is easier to interpret.

### **Advantages of MDS:**
- Works with any type of data as long as a distance measure can be computed.
- Provides a clear visual representation of complex relationships in the data.

### **Challenges:**
- Interpreting the principal coordinates can be more difficult than in PCA.
- MDS can be computationally expensive for large datasets.

MDS is particularly useful when we only have distance information and want to understand the structure of the data without needing to rely on raw feature values. In this section, we will apply MDS to our dataset to uncover its underlying patterns.

## Distance Metrics

When applying Multidimensional Scaling (MDS), selecting the appropriate distance metric is crucial because it directly impacts the MDS configuration and how relationships between data points are interpreted.

Different distance metrics capture different aspects of similarity or dissimilarity, which leads to varying representations of the data in the low-dimensional space. An inappropriate distance metric can distort these relationships, resulting in misleading conclusions.

For example, using **Euclidean distance** on a dataset with both quantitative and qualitative variables may not reflect the true dissimilarities, as it assumes continuous and scale-invariant data. In such cases, a metric like **Gower’s distance**, which can handle mixed data types, would be more appropriate.

### Why the Right Distance Metric Matters:
- **Data Type Compatibility:**
  - **Euclidean distance** is ideal for continuous, quantitative data.
  - **Matching coefficients** are used for binary data.
  - **Gower’s distance** is versatile, handling quantitative, binary, and categorical data.

- **Scale Invariance:**
  - **Euclidean distance** is **not scale-invariant**, meaning larger-scaled variables influence the distance more.
  - **Mahalanobis distance** is **scale-invariant**, adjusting for variance and correlations between variables.


### Comparison of Distance Metrics

#### Continuous Data
```{r, cache = TRUE}
# Euclidean distance
d_euclidean <- as.matrix(dist(continuous_data, method = "euclidean"))

# Manhattan distance
d_manhattan <- as.matrix(dist(continuous_data, method = "manhattan"))

# Canberra distance
d_canberra <- as.matrix(dist(continuous_data, method = "canberra"))

# Mahalanobis distance
sourceCpp("scripts/mahalanobis_dist.cpp")
D <- maha(as.matrix(continuous_data))
```

```{r, cache = TRUE}
# Plot the distance matrices in heatmaps using ggplot
p1 <- ggplot(melt(d_euclidean[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Euclidean distance") +
  theme(legend.position = "none") + 
  coord_fixed()


p2 <- ggplot(melt(d_manhattan[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Manhattan distance") +
  theme(legend.position = "none") + 
  coord_fixed()

p3 <- ggplot(melt(d_canberra[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Canberra distance") +
  theme(legend.position = "none") + 
  coord_fixed()


p4 <- ggplot(melt(D[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Mahalanobis distance") +
  theme(legend.position = "none") + 
  coord_fixed()

```

```{r, cache = TRUE}
# Set up a 2x2 layout for the plots
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

#### Binary Data

```{r}
# Jaccard distance
d_jaccard <- as.matrix(dist(binary_data, method = "binary"))

# Sokal-Michener distance
sokal_michener <- function(data) {
  n <- nrow(data)
  dist_matrix <- matrix(0, n, n)  # Initialize distance matrix
  
  for (i in 1:n) {
    for (j in i:n) {
      # Compare the two rows
      matches <- sum(data[i, ] == data[j, ])
      total <- ncol(data)
      distance <- 1 - (matches / total)
      
      # Fill the symmetric matrix
      dist_matrix[i, j] <- distance
      dist_matrix[j, i] <- distance
    }
  }
  
  return(dist_matrix)  # Return as a distance object
}

d_sokal_michener <- sokal_michener(as.matrix(binary_data))
```


```{r, cache = TRUE}
# Plot the distance matrices in heatmaps using ggplot
p1 = ggplot(melt(d_jaccard[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Jaccard distance") + 
  coord_fixed()


p2 = ggplot(melt(d_sokal_michener[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Sokal-Michener distance") + 
  coord_fixed()


# Set up a 1x2 layout for the plots
grid.arrange(p1, p2, ncol = 2)

```


#### Categorical Data


```{r}
# Dissimilarity coeffients (matching coefficients)
matching_coefficients <- function(data) {
  n <- nrow(data)
  dist_matrix <- matrix(0, n, n)  # Initialize distance matrix
  
  for (i in 1:n) {
    for (j in i:n) {
      # Compare the two rows
      matches <- sum(data[i, ] == data[j, ])
      total <- ncol(data)
      dissim_coef <- 1 - (matches / total)
      
      # Fill the symmetric matrix
      dist_matrix[i, j] <- dissim_coef
      dist_matrix[j, i] <- dissim_coef
    }
  }
  
  return(dist_matrix)  # Return as a distance object
}

# Other disssimilarity coefficients for categorical data
dissim_coefficients_6 <- function(data) {
  n <- nrow(data)
  dist_matrix <- matrix(0, n, n)  # Initialize distance matrix
  
  for (i in 1:n) {
    for (j in i:n) {
      # Compare the two rows
      matches <- sum(data[i, ] == data[j, ])
      total <- ncol(data)
      distance <- 1 - (matches / (matches + 2*(total - matches)))
      
      # Fill the symmetric matrix
      dist_matrix[i, j] <- distance
      dist_matrix[j, i] <- distance
    }
  }
  
  return(dist_matrix)  # Return as a distance object

}
```

```{r}
d_matching_coefficients <- matching_coefficients(as.matrix(categorical_data))
d_matching_coefficients_6 <- dissim_coefficients_6(as.matrix(categorical_data))
```

```{r}
# Plot the distance matrices in heatmaps using ggplot
p1 = ggplot(melt(d_matching_coefficients[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Matching coefficients") + 
  coord_fixed()

p2 = ggplot(melt(d_matching_coefficients_6[1:500,1:500]), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Dissimilarity coefficient") + 
  coord_fixed()
```

```{r}
grid.arrange(p1, p2, ncol = 2)
```


# Cluster analysis

Clustering is an unsupervised learning technique used to group data based on similarities. It does not require prior knowledge of the number of groups and is useful for discovering patterns, understanding relationships, and summarizing data. In this section, I will apply several clustering methods, including hierarchical and non hierarchical clustering, to identify meaningful clusters in the dataset.

- **Hierarchical Clustering:**  
  This method builds a hierarchy of clusters, either by starting with individual data points and merging them (agglomerative) or by starting with one large group and splitting it (divisive). Different linkage criteria (single, complete, and average) were used to calculate the distance between clusters, with results shown in dendrograms.

- **K-Means Clustering:**  
  K-means is a method that divides the data into a predefined number of clusters by minimizing the difference within each group. The optimal number of clusters was determined using the elbow method and silhouette analysis.

- **K-Medoids Clustering:**  
  K-medoids is similar to k-means, but it uses actual data points as the center of each cluster, making it more flexible with different distance measures. The PAM (Partitioning Around Medoids) algorithm was used to find the best clusters.

These techniques were chosen to explore different ways of grouping the data and to find meaningful patterns. Each method was applied with careful consideration of the dataset’s characteristics to ensure accurate and useful results.

## Hierarchical Clustering

```{r}
# dissimilarity matrix
d = daisy(data_no_revenue, metric = "gower")



# Hierarchical clustering
hc_single <- hclust(d, method = "single")
hc_complete <- hclust(d, method = "complete")
hc_average <- hclust(d, method = "average")

cophenetic_dist_single <- cophenetic(hc_single)
cophenetic_dist_complete <- cophenetic(hc_complete)
cophenetic_dist_average <- cophenetic(hc_average)

coph_corr_single <- cor(d, cophenetic_dist_single)
coph_corr_complete <- cor(d, cophenetic_dist_complete)
coph_corr_average <- cor(d, cophenetic_dist_average)

cat("Cophenetic Correlation Coefficient (Single Linkage):", coph_corr_single, "\n")
cat("Cophenetic Correlation Coefficient (Complete Linkage):", coph_corr_complete, "\n")
cat("Cophenetic Correlation Coefficient (Average Linkage):", coph_corr_average, "\n")

k = sqrt(nrow(data)/2)

plot(hc_single, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "", labels = FALSE)
rect.hclust(hc_single, k, border = "red")

plot(hc_complete, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "", labels = FALSE)
rect.hclust(hc_complete, k, border = "red")

plot(hc_average, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "", labels = FALSE)
rect.hclust(hc_average, k, border = "red")
```

```{r}
# Analyze the clusters by a specific variable (e.g., Revenue)
data_clusters <- data
data_clusters$cluster_single <- cutree(hc_single, k = k)
data_clusters$cluster_complete <- cutree(hc_complete, k = k)
data_clusters$cluster_average <- cutree(hc_average, k = k)



# Step 1: Calculate proportions for each cluster
proportions <- data_clusters %>%
  group_by(cluster_single, Revenue) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(cluster_single) %>%
  mutate(proportion = count / sum(count))

# Step 2: Create the bar plot
ggplot(proportions, aes(x = factor(cluster_single), y = proportion, fill = factor(Revenue))) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Revenue (Binary) by Cluster",
    x = "Cluster (cluster_single)",
    y = "Proportion",
    fill = "Revenue"
  ) +
  theme_minimal()


# Step 1: Calculate proportions for each cluster
proportions <- data_clusters %>%
  group_by(cluster_complete, Revenue) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(cluster_complete) %>%
  mutate(proportion = count / sum(count))

# Step 2: Create the bar plot
ggplot(proportions, aes(x = factor(cluster_complete), y = proportion, fill = factor(Revenue))) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Revenue (Binary) by Cluster",
    x = "Cluster (cluster_single)",
    y = "Proportion",
    fill = "Revenue"
  ) +
  theme_minimal()


# Step 1: Calculate proportions for each cluster
proportions <- data_clusters %>%
  group_by(cluster_average, Revenue) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(cluster_average) %>%
  mutate(proportion = count / sum(count))

# Step 2: Create the bar plot
ggplot(proportions, aes(x = factor(cluster_average), y = proportion, fill = factor(Revenue))) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Revenue (Binary) by Cluster",
    x = "Cluster (cluster_single)",
    y = "Proportion",
    fill = "Revenue"
  ) +
  theme_minimal()

```


## Non-Hierarchical Clustering

### K-prototypes Clustering


```{r}
# Range of k to try
k_values <- 2:25  

# Initialize vectors to store silhouette widths and WCSS values
sil_width <- numeric(length(k_values))
wcss <- numeric(length(k_values))

# Loop over different values of k to compute silhouette widths and WCSS
for (k in k_values) {
  # Run k-medoids with k clusters
  kproto_result <- kproto(data, type = "gower", k = k)  # Replace 'your_data' with your dataset
  
  # Compute the silhouette width
  sil <- silhouette(kproto_result$cluster, d)  # Compute silhouette
  sil_width[k - 1] <- mean(sil[, 3])  # Store the average silhouette width
  
  # Calculate WCSS (within-cluster sum of squares)
  wcss[k - 1] <- kproto_result$tot.withinss  # Total within-cluster sum of squares
}

# Plot the Silhouette Width for different k values
par(mfrow = c(1, 2))  # Set up 1 row, 2 columns for plots

plot(k_values, sil_width, type = "b", pch = 19, col = "blue",
     xlab = "Number of Clusters (k)", ylab = "Average Silhouette Width",
     main = "Silhouette Plot for Different k Values")

# Plot WCSS for different k values (Elbow Method)
plot(k_values, wcss, type = "b", pch = 19, col = "red",
     xlab = "Number of Clusters (k)", ylab = "WCSS",
     main = "Elbow Method for Optimal k")

# Find the optimal k based on the Silhouette plot
optimal_k_silhouette <- k_values[which.max(sil_width)]
cat("Optimal k based on Silhouette plot:", optimal_k_silhouette, "\n")

# Find the optimal k based on the Elbow method (visually identify elbow)
# You can manually inspect the "elbow" point in the WCSS plot

```

```{r}
kproto_result <- kproto(data, type = "gower", k = 7)
```


```{r}
data_clusters$cluster_kproto <- kproto$cluster

# Step 1: Calculate proportions for each cluster
proportions <- data_clusters %>%
  group_by(kproto, Revenue) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(kproto) %>%
  mutate(proportion = count / sum(count))

# Step 2: Create the bar plot
ggplot(proportions, aes(x = factor(kproto), y = proportion, fill = factor(Revenue))) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Revenue (Binary) by Cluster",
    x = "Cluster (cluster_single)",
    y = "Proportion",
    fill = "Revenue"
  ) +
  theme_minimal()

```

### K-medoids Clustering

```{r}
# Range of k to try
k_values <- 2:25  

# Initialize vectors to store silhouette widths and WCSS values
sil_width <- numeric(length(k_values))

# Loop over different values of k to compute silhouette widths and WCSS
for (k in k_values) {
  # Run k-medoids with k clusters
  kmedoids_result <- pam(d, k = k)  # Replace 'your_data' with your dataset
  # Compute the silhouette width
  sil <- silhouette(kmedoids_result$cluster, d)  # Compute silhouette
  sil_width[k - 1] <- mean(sil[, 3])  # Store the average silhouette width
  
  # Calculate WCSS (within-cluster sum of squares)
}

# Plot the Silhouette Width for different k values
par(mfrow = c(1, 2))  # Set up 1 row, 2 columns for plots

plot(k_values, sil_width, type = "b", pch = 19, col = "blue",
     xlab = "Number of Clusters (k)", ylab = "Average Silhouette Width",
     main = "Silhouette Plot for Different k Values")

# Find the optimal k based on the Silhouette plot
optimal_k_silhouette <- k_values[which.max(sil_width)]
cat("Optimal k based on Silhouette plot:", optimal_k_silhouette, "\n")

# Find the optimal k based on the Elbow method (visually identify elbow)
# You can manually inspect the "elbow" point in the WCSS plot
```


```{r}
data_clusters$cluster_kmedoids <- kmedoids$clustering

# Step 1: Calculate proportions for each cluster
proportions <- data_clusters %>%
  group_by(cluster_kmedoids, Revenue) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(cluster_kmedoids) %>%
  mutate(proportion = count / sum(count))

# Step 2: Create the bar plot
ggplot(proportions, aes(x = factor(cluster_kmedoids), y = proportion, fill = factor(Revenue))) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Revenue (Binary) by Cluster",
    x = "Cluster (cluster_single)",
    y = "Proportion",
    fill = "Revenue"
  ) +
  theme_minimal()
```



