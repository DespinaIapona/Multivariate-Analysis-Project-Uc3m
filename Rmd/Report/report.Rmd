---
title: "Analysis of Online Shopping Behavior"
author: "Despoina Iapona & Gerard Palomo"
output:
  pdf_document:
    toc: true
    toc_depth: 3
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, out.height = '70%', out.width = '70%', fig.align = "center", comment = "#>")
```

```{r}
library(ggplot2)   
library(reshape2)  
library(dplyr)     
library(pander)
library(knitr)
library(gridExtra)
library(GGally)
library(caret)
library(tidyr)
library(kableExtra)
```

\newpage
# 1. Dataset Description

## 1.1 Overview
The **Online Shoppers Purchasing Intention Dataset** is sourced from the **UCI Machine Learning Repository** and provides valuable insights into online shopping behavior. This dataset tracks user interactions on an e-commerce website, with the primary goal of predicting whether a shopper will make a purchase during their visit.

It includes features such as the number of pages visited, time spent on various types of pages, bounce rates, and exit rates. Additionally, the dataset captures demographic information about the shoppers, such as whether they are new or returning visitors. The target variable indicates whether the shopper made a purchase, making the dataset a valuable resource for understanding and modeling purchasing behavior.

The original dataset contains **12,330 records** and **18 features**, including both numerical and categorical variables. For our analysis, we've taken a subset of the original dataset which contains **2,000 records** and **18 features**. 

## 1.2 Features
Below is a table representation with all the key features:

| **Feature Name**        | **Type**     | **Description**                                                   |
|-------------------|-----------------|------------------------------------|
| Administrative          | Quantitative | Number of administrative pages visited                            |
| AdministrativeDuration  | Quantitative | Time spent on administrative pages in seconds                     |
| Informational           | Quantitative | Number of informational pages visited                             |
| InformationalDuration   | Quantitative | Time spent on informational pages in seconds                      |
| ProductRelated          | Quantitative | Number of product-related pages visited                           |
| ProductRelatedDuration  | Quantitative | Time spent on product-related pages in seconds                    |
| BounceRates             | Quantitative | Percentage of visitors leaving after one page                     |
| ExitRates               | Quantitative | Percentage of sessions exiting from each page                     |
| PageValues              | Quantitative | Average value attributed to a page                                |
| SpecialDay              | Quantitative | Metric indicating proximity to significant holidays               |
| Weekend                 | Binary       | Indicates if the session occurred on a weekend (1 = Yes, 0 = No)  |
| Revenue                 | Binary       | Indicates if the session resulted in a purchase (1 = Yes, 0 = No) |
| VisitorType             | Categorical  | Visitor category (e.g., Returning, New, Other)                    |
| Month                   | Categorical  | Month of the visit (e.g., Jan, Feb)                               |
| OperatingSystems        | Categorical  | Visitor operating system (e.g., Windows, MacOS)                   |
| Browser                 | Categorical  | Browser used by the visitor (e.g., Safari, Chrome)                |
| Region                  | Categorical  | Visitor geographical region                                       |
| TrafficType             | Categorical  | Type of traffic source leading to the visit                       |

To obtain this structure a preprocessing step was performed, which included the following transformations:
  - The categorical variables were converted to factors.
  - The target variable (0 for no purchase, 1 for purchase) and the binary variables were converted to a binary format 
  - Name changes were made to some variables for better readability and naming consistency.
  - A random subset of 2,000 observations was selected from the original dataset.


The code to preprocess the dataset is as follows:

```{r, eval=FALSE, echo=TRUE}
set.seed(100)
data <- read_csv(
  paste('../../data/online+shoppers+purchasing+intention+dataset/',
        'online_shoppers_intention.csv', sep = '')
)

data <- data %>%
  mutate(
    Month = as.factor(Month),
    Region = as.factor(Region),
    TrafficType = as.factor(TrafficType),
    VisitorType = as.factor(VisitorType),
    OperatingSystems = as.factor(OperatingSystems),
    Browser = as.factor(Browser),
    Weekend = as.factor(as.numeric(Weekend == 'TRUE')),
    Revenue = as.factor(as.numeric(Revenue == 'TRUE')),
    SpecialDay = as.factor(SpecialDay)
  ) %>%
  rename(
    AdministrativeDuration = Administrative_Duration,
    InformationalDuration = Informational_Duration,
    ProductRelatedDuration = ProductRelated_Duration
  ) %>% 
  sample_n(2000)
```



## 1.3 Data Integrity
We begin by assessing the integrity of the dataset by analyzing the presence of missing data across its features. The table below presents the count of missing values for each feature. As shown, there are **no missing values** in any of the features, which indicates that the dataset is complete and does not require any imputation for missing data.

Additionally, we check for duplicate records in the dataset. The analysis revealed **6 duplicate rows**, which may be due to several factors. These duplicates could represent unintentional repetitions of the same individualâ€™s data, or they could arise from issues during data collection, such as individuals' data being recorded more than once under slightly different conditions. As the amount is relatively small, we will proceed with the analysis without removing these duplicates. 

Given the absence of missing values and the limited number of duplicate rows, the dataset does not require any further cleaning or preprocessing steps before proceeding with our initial analysis.

```{r}
data <- readRDS('../../data/R_datasets/processed_dataset.rds')

# Checking for missing values
missing_values <- sapply(data, function(x) sum(is.na(x)))

# Checking for duplicate rows
duplicate_rows <- sum(duplicated(data))

# Displaying the missing values count
kable(missing_values, caption = "Missing Values Count for Each Feature", align = "c")

# Displaying the number of duplicate rows
cat("Number of duplicate rows:", duplicate_rows)
```
\vspace{0.3cm}

# 2. Univariate Analysis

## 2.1 Numeric Variables

```{r}
# Split numeric and categorical variables
numeric_vars <- data %>% dplyr::select(where(is.numeric))
categorical_vars <- data %>% dplyr::select(where(is.factor))
```

### 2.1.1 Time Related Variables

The duration variables (e.g., `AdministrativeDuration`, `InformationalDuration`, `ProductRelatedDuration`) exhibit values for the mean significantly higher than the median, indicating a heavily right-skewed distribution. This suggests that there are a few sessions with very high durations that are pulling the mean upwards. The range of values for these variables is quite large, with the maximum values being several times higher than the 75th percentile. 
```{r, out.width = '100%', out.height = '100%'}
# Summarize numeric variables
numeric_summary <- summary(numeric_vars[, c("AdministrativeDuration", "InformationalDuration", "ProductRelatedDuration")])

# Display the summary aligned to the left
pander(numeric_summary, caption = "Summary of Numeric Time Related Variables", style = "rmarkdown", justify = "left")

```

The boxplots and histograms provide clear evidence of the right-skewed nature of these variables, characterized by a high concentration of values near zero and a long tail of extreme values. This distribution highlights the presence of numerous outliers, which are not isolated anomalies but rather a significant portion of the data. These extreme values likely represent important behavioral patterns or user interactions that could offer valuable insights. Therefore, instead of removing or treating them as noise, we choose to retain these outliers in our analysis to ensure a comprehensive understanding of the dataset and its implications.

```{r, out.height = '100%', out.width = '100%'}
# Set layout for 3 rows and 3 columns (9 plots per page)
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))  # Adjust margins

# Create histograms for each numeric variable
for (var_name in names(numeric_vars[, c("AdministrativeDuration", "InformationalDuration", "ProductRelatedDuration")])) {
  
  # Histogram
  hist(numeric_vars[[var_name]], 
       main = var_name, 
       xlab = var_name, 
       col = "lightgreen", 
       breaks = 50)
  
  # Boxplot
  boxplot(numeric_vars[[var_name]], 
          main = paste(var_name), 
          ylab = var_name, 
          col = "lightblue", 
          horizontal = TRUE)
}
```

### 2.1.2 Page Interaction Variables

The variables related to page interactions (e.g., `Administrative`, `Informational`, `ProductRelated`) represent the number of pages visited by the user during the session. From all 3 variables, `ProductRelated` has the highest mean and median values while `Informational` and `Administrative` have lower but similar values. The summary statistics also suggest a right-skewed distribution for these variables, with the mean exceeding the median. 
```{r, out.width = '100%', out.height = '100%'}
# Summarize numeric variables
numeric_summary <- summary(numeric_vars[, c("Administrative", "Informational", "ProductRelated")])

# Display the summary aligned to the left
pander(numeric_summary, caption = "Summary of Page Interaction Variables", style = "rmarkdown", justify = "left")

```

The boxplots and histograms for these variables confirm the right-skewed distribution, with a large number of sessions having low page interaction counts and a few sessions with very high counts. While all three variables exhibit similar patterns, the `Informational` variable has a more pronounced skewness, followed by `ProductRelated` and `Administrative`. The presence of outliers in these variables is expected, as user behavior can vary significantly, with some users exploring multiple pages while others may exit quickly after viewing a few pages. We believe this outlier behavior is essential for understanding user engagement and purchase intent and should be retained in the analysis.


```{r, out.height = '100%', out.width = '100%'}
# Set layout for 3 rows and 3 columns (9 plots per page)
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))  # Adjust margins

# Create histograms for each numeric variable
for (var_name in names(numeric_vars[, c("Administrative", "Informational", "ProductRelated")])) {
  
  # Histogram
  hist(numeric_vars[[var_name]], 
       main = var_name, 
       xlab = var_name, 
       col = "lightgreen", 
       breaks = 50)
  
  # Boxplot
  boxplot(numeric_vars[[var_name]], 
          main = paste(var_name), 
          ylab = var_name, 
          col = "lightblue", 
          horizontal = TRUE)
}
```

### 2.1.3 Other Numeric Variables

The following variables are derived from Google Analytics, providing insights into user behavior on the website:

- **BounceRates**: Represents the percentage of visitors who leave the site after viewing only one page. Summary statistics show values ranging from 0 to 0.2, with a mean of 0.023 and a median of 0.003. The distribution is right-skewed, as the mean is higher than the median, indicating a concentration of low values and a few higher values.

- **ExitRates**: Represents the percentage of visitors who exit the site from a specific page. While the dataset doesn't specify which page this refers to, it may be a critical page or one leading to a purchase. The values range from 0 to 0.2, with a mean of 0.043 and a median of 0.025. Like **BounceRates**, this variable is also right-skewed although not as extreme,  with a concentration of low values and a few higher values.

- **PageValues**: Represents the average value of a page that a user visits before completing a transaction. The values range from 0 to 255, with a mean of 5.89 and a median of 0.0. This variable is heavily right-skewed, indicating that while most pages have little to no assigned value. There are a few pages with high values that significantly impact the mean.


```{r, out.width = '100%', out.height = '100%'}
# Summarize numeric variables
numeric_summary <- summary(numeric_vars[, c("BounceRates", "ExitRates", "PageValues")])

# Display the summary aligned to the left
pander(numeric_summary, caption = "Summary of Page Interaction Variables", style = "rmarkdown", justify = "left")

```
\vspace{0.3cm}

The visualizations for these variables further confirm the right-skewed distribution, with a concentration of low values and a few high values. The histograms and boxplots provide a clear representation of the distribution of these variables, highlighting the presence of outliers and the need to consider these extreme values in the analysis. We've observed outliers in all variables which may suggest specific user behaviors or interactions that could be crucial for understanding purchasing intent and user engagement on the website. As done with other variables, we choose to retain these outliers in our analysis to ensure a comprehensive understanding of the dataset.

\vspace{0.3cm}

```{r, out.height = '100%', out.width = '100%'}
# Set layout for 3 rows and 3 columns (9 plots per page)
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))  # Adjust margins

# Create histograms for each numeric variable
for (var_name in names(numeric_vars[, c("BounceRates", "ExitRates", "PageValues")])) {
  
  # Histogram
  hist(numeric_vars[[var_name]], 
       main = var_name,, 
       xlab = var_name, 
       col = "lightgreen", 
       breaks = 50)
  
  # Boxplot
  boxplot(numeric_vars[[var_name]], 
          main = paste(var_name), 
          ylab = var_name, 
          col = "lightblue", 
          horizontal = TRUE)
}
```
### 2.1.4 Transformations

Given the right-skewed nature of the numeric variables, we decided to try and apply transformation to achive a more normal distribution by reducing the positive skewness. Due to the extreme values and presence of zeros in the data, we opted for the **Box-Cox transformation** along with shifting the data by adding a constant value to avoid issues with zero values. This transformation did not result in a significant improvement in normality, as the original data was already heavily skewed.

The 2 histograms below show a comparison of the original and transformed data for the `ProductRelatedDuration` and `BounceRates` variables. The histograms illustrate that 2 scenarios we observed in all our numerical variables. In one case, the presence of a large number of zero values results in a spike at zero in the transformed data. In the other case, the transformation does not significantly alter the distribution of the data, as the original data is extremely skewed.

It is because of these reasons that we decided to retain the original data for our analysis, as the transformation did not provide a substantial improvement in normality and also reduced the interpretability of the data. The presence of outliers and extreme values in the original data is essential for understanding user behavior and engagement on the website, and we believe that these values should be retained in our analysis.


```{r, include=FALSE}
numeric_vars_shifted <- numeric_vars + 1

print(numeric_vars_shifted)

preProc <- preProcess(numeric_vars_shifted, method = "BoxCox")
# View the transformed data
transformed_data <- predict(preProc, newdata = numeric_vars_shifted)
print(transformed_data)
```

```{r}
# Set up the plot layout to show side-by-side histograms
par(mfrow = c(2, 2))
# Loop over each column (variable) to plot histograms for both original and transformed data
for (col in colnames(numeric_vars[,c("ProductRelatedDuration", "BounceRates")])) {
  # Plot original data
  hist(numeric_vars[[col]], main = paste("Original", col), 
       xlab = col, col = "lightblue", border = "black", breaks  = 50)

  # Plot transformed data
  hist(transformed_data[[col]], main = paste("Transformed", col), 
       xlab = col, col = "lightgreen", border = "black", breaks = 50)
}

# Reset plotting layout to default
par(mfrow = c(1, 1))
```

## 2.2 Categorical Variables

### 2.2.1 Visitor Demographics/Identity

- **VisitorType:**  
  The majority of visitors are **Returning Visitors**, indicating a strong base of repeat users. **New Visitors** form a smaller proportion, and the **Other** category is negligible, suggesting minimal contribution from other visitor types.

- **Region:**  
  Region **1** has the highest count of visitors, followed by Region **3**. Traffic is highly concentrated in these specific regions, with other regions showing a relatively lower number of visitors.


```{r}
# Summarize categorical variables
categorical_summary <- summary(categorical_vars[, c("VisitorType", "Region")])

# Create barplots and store them in a list
plots <- lapply(names(categorical_vars[, c("VisitorType", "Region")]), function(var_name) {
  ggplot(categorical_vars, aes(x = .data[[var_name]])) +
    geom_bar(fill = "lightblue") +
    labs(title = var_name, 
         x = var_name, 
         y = "Count") +
    theme_minimal()
})

# Display plots in a 1x2 grid layout
grid.arrange(grobs = plots, ncol = 2)
```

### 2.2.2 Technical Attributes

- **Browser:**  
  Browser **2** is the most commonly used browser, significantly surpassing others in popularity. Browser **1** has a notable user base, while the other browsers are used by only a small fraction of users.

- **OperatingSystems:**  
  Operating System **2** dominates usage, followed by Operating System **1**. Other operating systems have a minor presence, highlighting user preference for a few dominant operating systems.

```{r}
# Create barplots and store them in a list
plots <- lapply(names(categorical_vars[, c("Browser", "OperatingSystems")]), function(var_name) {
  ggplot(categorical_vars, aes(x = .data[[var_name]])) +
    geom_bar(fill = "lightblue") +
    labs(title = var_name, 
         x = var_name, 
         y = "Count") +
    theme_minimal()
})

# Display plots in a 1x2 grid layout
grid.arrange(grobs = plots, ncol = 2)
```
### 2.2.3 Traffic and Source

- **Month:**  
  Traffic peaks in **March**, **May**, and **November**, with the highest activity in **May**. Lower traffic is observed in **June**, **July**, and **February**, potentially reflecting seasonal trends or business cycles.

- **TrafficType:**  
  TrafficType **2** is the primary source of traffic, with a significant lead over TrafficType **1**. Other TrafficTypes contribute marginally, indicating that a few referral sources or marketing channels drive the majority of traffic.

```{r}
# Create barplots and store them in a list
plots <- lapply(names(categorical_vars[, c("Month", "TrafficType")]), function(var_name) {
  ggplot(categorical_vars, aes(x = .data[[var_name]])) +
    geom_bar(fill = "lightblue") +
    labs(title = var_name, 
         x = var_name, 
         y = "Count") +
    theme_minimal()
})

# Display plots in a 1x2 grid layout
grid.arrange(grobs = plots, ncol = 2)
```
## 2.3 Binary Variables

- **Weekend:**  
  The  binary variable `Weekend` indicates whether the session occurred on a weekend. We observe that around ~23% of the sessions occurred on weekends, while the majority (~77%) took place on weekdays. This distribution suggests that the website receives a higher volume of traffic on weekdays compared to weekends (which also holds if we took the daily average).
  
- **Revenue:**  
  The target variable `Revenue` indicates whether a session resulted in a purchase. The dataset is imbalanced, with a higher number of sessions where no purchase (~85%) was made compared to sessions resulting in a purchase (~15%). This imbalance is expected in e-commerce datasets, where the conversion rate is typically lower than the non-conversion rate.

```{r out.width = '70%', out.height = '70%'}
# Create barplots and store them in a list
plots <- lapply(names(categorical_vars[, c("Weekend", "Revenue")]), function(var_name) {
  ggplot(categorical_vars, aes(x = .data[[var_name]])) +
    geom_bar(fill = "lightblue") +
    labs(title = var_name, 
         x = var_name, 
         y = "Count") +
    theme_minimal()
})

# Display plots in a 1x2 grid layout
grid.arrange(grobs = plots, ncol = 2)
```

# 3. Multivariate Analysis

## 3.1 Correlation Analysis

The correlation matrix provides insights into the relationships between the numeric variables in the dataset. The heatmap below visualizes the correlation matrix, with values ranging from -1 to 1. A value of 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation. We can observe  3 distinct clusters of variables:

- Page Related Variables like `Administrative`, `Informational`, `ProductRelated`, `AdministrativeDuration`, `InformationalDuration` and `ProductRelatedDuration`are moderately positively correlated with each other.  
- `BounceRates` and `ExitRates` are highly positively correlated with each other and negatively correlated with the rest of the variables.  
- `PageValues` shows very little to no correlation with the rest of the variables.  
 
```{r, out.width = '90%', out.height = '90%'}
# Compute the correlation matrix
correlation_matrix <- cor(numeric_vars)

correlation_melted <- melt(correlation_matrix)

# Heatmap of the correlation matrix with values
ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +  # Create the heatmap tiles
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +  # Add the correlation values inside the tiles
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +  # Color scale
  labs(title = "Correlation Matrix Heatmap", x = "Variables", y = "Variables") +  # Title and labels
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-ax

```


## 3.2 Pairwise Conditional Scatter Plots

This conditional pairwise scatter plots provide a visual representation of the relationships between numeric variables, colored by the `Revenue` variable. We observe what we've already seen in the correlation matrix and in the univariate analysis. We see clearly skewed distributions for all variables due to the nature of the data, and as seen in the correlation matrix, we do not observe any clear linear relationships between the variables except for `BounceRates` and `ExitRates` which are significantly positively correlated between them.

Another interesting observation is that `Revenue` does not seem to have a clear separation between the two classes in the scatter plots. This suggests that the numeric variables alone may not be sufficient to predict the `Revenue` variable. 

```{r, out.width = '100%', out.height = '100%'}
# Pairwise scatter plots for numeric variables colored by Revenue
ggpairs(
  data[, colnames(numeric_vars)], 
  lower = list(continuous = wrap("points", alpha = 0.5, size = 0.1)),
  upper = list(continuous = wrap("cor", size = 2)),
  mapping = aes(color = data$Revenue),
  title = "Pairwise Conditional Scatter Plots of Numeric Variables"
  ) + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
    axis.text.y = element_text(hjust = 1, size = 6),
    strip.text.x = element_text(size = 6, angle = -90), 
    strip.text.y = element_text(size = 6, angle = 0),    
    axis.text = element_text(size = 6, angle = -45)     
    )
```
## 3.3 Conditional Barplots

The conditional barplots below show the proportion of sessions resulting in a purchase (`Revenue`) for each categorical variable. The plots reveal little variation in the proportion of purchases across different categories within each variable. This suggests that the categorical variables alone may not be sufficient to predict the purchase intent of a session. However, these variables could still provide valuable insights when combined with other features in a predictive model. 
The only variables that seem to have some variation in the proportion of purchases are `VisitorType`, `Month` and `TrafficType` where some of the categories have a higher proportion of purchases than others. For example, in the `VisitorType` variable, `Returning_Visitor` shows a lower proportion of purchases while the `New_Visitor` category has a higher proportion of purchases.

```{r}
# Reshape the data into long format
categorical_vars_long <- categorical_vars %>%
  pivot_longer(cols = -Revenue, names_to = "Variable", values_to = "Value")

# Create the plot
ggplot(categorical_vars_long, aes(x = Value, fill = Revenue)) +
  geom_bar(aes(y = ..prop.., group = Revenue), position = "dodge") +
  labs(title = "Category Proportions by Revenue", x = "Category", y = "Proportion") +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal() +
  theme(
    legend.position = "top",  # Shared legend at the top
    axis.text = element_text(size = 6, angle = 45)  # Adjust axis text size and rotation
  ) +
  facet_wrap(~ Variable, scales = "free", ncol = 4)  # Facet by 'Variable' (original column names) with 4 columns


```


# 4. Principal Component Analysis

Principal Component Analysis (PCA) was applied to reduce the dimensionality of the dataset while retaining as much variance as possible. This technique is instrumental in identifying key patterns in the data and visualizing its underlying structure.

## 4.1 Methodology

After initial data preprocessing and analysis, we observed that the dataset contains numerical variables with differing scales. Given the nature of the data, we opted not to apply additional transformations, as explained previously. To address the issue of differing scales, we applied PCA using the correlation matrix instead of the covariance matrix. This approach standardizes the variables to have a mean of 0 and a standard deviation of 1, ensuring comparability across variables.

While we previously noted that the numerical variables are not highly correlated pairwise, it is also valuable to evaluate intercorrelation measures for the entire dataset. PCA performs optimally when the data exhibits strong intercorrelation, so these measures will provide insight into how effective PCA is likely to be for this dataset.

```{r}
intercorrelations <- function(X) {
  n <- nrow(X)
  p <- ncol(X)
  R <- cor(X)  
  q <- numeric(6)
  lambda <- eigen(R)$values
  rjj <- diag(solve(R))
  
  q[1] <- (1 - min(lambda) / max(lambda))^(p + 2)
  q[2] <- 1 - p / sum(1 / lambda)
  q[3] <- 1 - sqrt(det(R))
  q[4] <- (max(lambda) / p)^(3 / 2)
  q[5] <- (1 - min(lambda) / p)^5
  q[6] <- sum((1 - 1 / rjj) / p)
  
  results <- data.frame(
    Measure = c(
      "Multivariate Dispersion",
      "KMO-like Measure",
      "Bartlettâ€™s Determinant Test",
      "Multivariate Kurtosis",
      "Multicollinearity Index",
      "Average Variable Dependency"
    ),
    Value = q
  )
  
  pander(
    results,
    caption = "Intercorrelation Measures",
    justify = "left"
  )
  
}
```

```{r}
intercorrelations(numeric_vars)
```

The intercorrelation measures indicate that the dataset exhibits moderate to high intercorrelation among variables. This suggests that applying PCA is appropriate, as it will effectively reduce dimensionality while preserving most of the variance in the data. The results highlight a sufficient level of correlation to justify PCA, with some redundancy among variables that PCA can address.

## 4.2 Results

The following table displays the principal components along with the variance explained by each component. The first principal component explains the highest proportion of variance, followed by subsequent components. We observe how the 1st and 2nd component explain more than half of the variance in the data, indicating that these components capture the most significant patterns in the dataset. The subsequent components explain less but still a significant amount of variance. 

If we analyize more closely the first principal component we can get some insights from it. All variables except `BounceRates` (-0.2501) and `ExitRates` (0.2902) have positive coefficients. The variables with the highest coefficients are `ProductRelated` (0.4174) and  `ProductRelatedDuration` (0.4171), indicating that these variables contribute the most to the first principal component. This suggests that user interactions with product-related pages is a key factor in explaining the variance in the dataset. On the other side, `PageValues` has a very low coefficient (0.0761), indicating that it's impact on the first principal component is relatively low compared to the other variables.

\scriptsize
```{r}
# Perform PCA
pca <- prcomp(numeric_vars, center = TRUE, scale. = TRUE)

# Calculate variance explained by each principal component
explained_variance <- pca$sdev^2
variance_explained <- explained_variance / sum(explained_variance) * 100

# Create a new row with the variance explained
variance_row <- c(variance_explained)

# Add the row to the PCA rotation table
pca_table <- rbind(pca$rotation, "Variance Explained" = variance_row)

# Create kable table and highlight the last row
kable(pca_table, caption = "Principal Components with Variance Explained", align = "l", digits = 4) %>%
  kable_styling() %>%
  row_spec(nrow(pca_table), bold = TRUE) %>%
  row_spec(nrow(pca_table) - 1,  extra_latex_after = "\\hline \\hline")



```
\normalsize
```{r, eval=FALSE}
# # PCA step by step
# eig_result <- eigen(correlation_matrix)
# 
# # Get the eigenvalues and eigenvectors
# Lambda <- eig_result$values  # Eigenvalues
# T <- eig_result$vectors      # Eigenvectors
# 
# # Sort eigenvalues and eigenvectors in descending order
# sorted_indices <- order(Lambda, decreasing = TRUE)  # Indices for sorting
# 
# # Apply the sorting
# Lambda_sorted <- Lambda[sorted_indices]  # Sorted eigenvalues
# T_sorted <- T[, sorted_indices]          # Sorted eigenvectors
# 
# # Show the sorted eigenvalues
# Lambda_sorted
# 
# # Show the eigenvectors
# T_sorted

```

## 4.3 Principal Component Selection

The primary objective of PCA is to reduce the dimensionality of the dataset while retaining as much variance as possible. There are several methods to determine the number of principal components to retain, such as the **Kaiser criterion**, **scree plot**, and **cumulative variance explained**. Our dataset only contains 9 numerical variables so the tradeoff between the number of components and the variance explained is not as critical as in datasets with a larger number of variables. Losing some variance in exchange for a simpler model may not be worth it in this case.

That said, our original dataset contained more than 12,000 records, so maybe losing some variance in exchange for a smaller model could be worth it in terms of computational efficiency when working with the full dataset. In this case we could fix some threshold for the cumulative variance explained and select the number of components that meet that threshold. As we mentioned before, dimensional reduction is not as critical in this case, so we will fix a relatively high threshold of 95% of variance explained. 

```{r}
cumsum_variance <- cumsum(variance_explained)

ggplot(data = data.frame(Principal_Component = 1:length(variance_explained), 
                         Cumulative_Variance = cumsum_variance), 
       aes(x = Principal_Component, y = Cumulative_Variance)) +
  geom_line(color = "blue") +
  geom_point(color = "red", alpha = 0.6) +
  scale_x_continuous(breaks = 1:9) + # Note the added "+" here
  labs(title = "Cumulative Variance Explained by Principal Components", 
       x = "Principal Component", 
       y = "Cumulative Variance Explained (%)") +
  geom_line(aes(x = Principal_Component, y = 95), linetype = "dashed", color = "red") + 
  geom_point(aes(x = 7, y = cumsum_variance[7]), color = "red", size = 5) +
  theme_minimal()
```
The threshold of 95% of variance explained is reached by taking the seven first principal components. This means that these seven components capture at least 95% of the variance in the dataset.


# Packages and Tools

The analysis was performed using **R**, with the report generated in **RMarkdown**. For data wrangling, the `dplyr`, `reshape2` and `tidyr` libraries were utilized. Visualizations were created using `ggplot2`, `GGally`, and `gridExtra`. The report was compiled and formatted with `knitr`, `pander` and `kableExtra`, while `caret` was employed for the Principal Component Analysis. 


# References

UCI Machine Learning Repository. (n.d.). *Online Shoppers Purchasing Intention Dataset*. Retrieved from [https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset).

GranÃ©, A. (2024). *Multivariate Analysis: 1. Multidimensional Datasets*. Master in Statistics for Data Science, Universidad Carlos III de Madrid. Retrieved from [aurea.grane@uc3m.es](mailto:aurea.grane@uc3m.es). Licensed under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0.

GranÃ©, A. (2024). *Multivariate Analysis: 2. Principal Component Analysis (PCA)*. Master in Statistics for Data Science, Universidad Carlos III de Madrid. Retrieved from [aurea.grane@uc3m.es](mailto:aurea.grane@uc3m.es). Licensed under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0.

Yihui, X. (2018). *R Markdown Cookbook*. Retrieved from [https://bookdown.org/yihui/rmarkdown-cookbook/](https://bookdown.org/yihui/rmarkdown-cookbook/).

R Graph Gallery. (n.d.). *R Graph Gallery: A collection of graphs made with R*. Retrieved from [https://www.r-graph-gallery.com/](https://www.r-graph-gallery.com/).

